{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "084c5f26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
    "os.environ[\"LANGSMITH_API_KEY\"] = os.getenv(\"LANGSMITH_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8c627c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "if not os.environ.get(\"GOOGLE_API_KEY\"):\n",
    "  os.environ[\"GOOGLE_API_KEY\"] = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"gemini-2.5-flash\", model_provider=\"google_genai\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9ad24f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "embeddings = GoogleGenerativeAIEmbeddings(model=\"models/gemini-embedding-001\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4754ec62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(\n",
    "    collection_name=\"example_collection\",\n",
    "    embedding_function=embeddings,\n",
    "    persist_directory=\"./chroma_langchain_db\",  # Where to save data locally, remove if not necessary\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bac99078",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\tejus\\OneDrive\\Desktop\\Full_Stack\\ML\\RAG_Chatbot\\rag_env\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No languages specified, defaulting to English.\n",
      "No. of docs = 1\n",
      "mprehensive, well-structured guide to the essential aspects of building generative AI systems. A must-read for any professional looking to scale AI across the enterprise.” Vittorio Cretella, former global CIO at P&G and Mars\n",
      "\n",
      "“Chip Huyen gets generative AI. She is a remarkable teacher and writer whose work has been instrumental in helping teams bring AI into production. Drawing on her deep experti\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import DirectoryLoader\n",
    "\n",
    "loader = DirectoryLoader(\n",
    "    path=r\"C:\\Users\\tejus\\OneDrive\\Desktop\\Full_Stack\\ML\\RAG_Chatbot\\rag_env\\data\",\n",
    "    glob=\"*.pdf\"\n",
    ")\n",
    "\n",
    "docs = loader.load()\n",
    "\n",
    "print(f\"No. of docs = {len(docs)}\")\n",
    "print(docs[0].page_content[100:500])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ce56d8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split data into 1425 sub-documents.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "  chunk_size = 1000,\n",
    "  chunk_overlap = 200,\n",
    "  add_start_index = True\n",
    ")\n",
    "\n",
    "splits = text_splitter.split_documents(docs)\n",
    "\n",
    "print(f\"Split data into {len(splits)} sub-documents.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c014f338",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['d72ed5bc-cf87-4265-9a64-5fde351a56dd', '3a981f8c-0f72-4f30-9aca-2774d3d36755', '7e5d4628-bb21-48bb-9a6e-93caf7ac5100']\n"
     ]
    }
   ],
   "source": [
    "document_ids = vector_store.add_documents(documents=splits)\n",
    "\n",
    "print(document_ids[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00d5c53e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\n",
      "Question: (question goes here) \n",
      "Context: (context goes here) \n",
      "Answer:\n"
     ]
    }
   ],
   "source": [
    "from langchain import hub\n",
    "\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "example_messages = prompt.invoke(\n",
    "    {\"context\": \"(context goes here)\", \"question\": \"(question goes here)\"}\n",
    ").to_messages()\n",
    "\n",
    "assert len(example_messages) == 1\n",
    "print(example_messages[0].content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3d2a068f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "from typing_extensions import List, TypedDict\n",
    "\n",
    "\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37140237",
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"])\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"question\"], \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93c0be7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.graph import START, StateGraph\n",
    "\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fa4eeb45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: Explain Finetuning?\n",
      "Context: [Document(id='0bd14be2-5e2b-4255-b584-fdade8f27878', metadata={'start_index': 674694, 'source': 'C:\\\\Users\\\\tejus\\\\OneDrive\\\\Desktop\\\\Full_Stack\\\\ML\\\\RAG_Chatbot\\\\rag_env\\\\data\\\\AI Engineering Building Applications with Foundation Models (Chip Huyen) (Z-Library).pdf'}, page_content='There’s a lot to discuss. Let’s dive in!\\n\\nFinetuning Overview To finetune, you start with a base model that has some, but not all, of the capabilities you need. The goal of finetuning is to get this model to perform well enough for your specific task.\\n\\nFinetuning is one way to do transfer learning, a concept first introduced by Bozinov‐ ski and Fulgosi in 1976. Transfer learning focuses on how to transfer the knowledge gained from one task to accelerate learning for a new, related task. This is conceptu‐ ally similar to how humans transfer skills: for example, knowing how to play the piano can make it easier to learn another musical instrument.\\n\\nAn early large-scale success in transfer learning was Google’s multilingual translation system (Johnson et. al, 2016). The model transferred its knowledge of Portuguese– English and English–Spanish translation to directly translate Portuguese to Spanish, even though there were no Portuguese–Spanish examples in the training data.'), Document(id='4f25c021-e4dd-46f5-807e-a9abb1aee87e', metadata={'source': 'C:\\\\Users\\\\tejus\\\\OneDrive\\\\Desktop\\\\Full_Stack\\\\ML\\\\RAG_Chatbot\\\\rag_env\\\\data\\\\AI Engineering Building Applications with Foundation Models (Chip Huyen) (Z-Library).pdf', 'start_index': 123150}, page_content='On the Differences Among Training, Pre-Training, Finetuning, and Post-Training Training always involves changing model weights, but not all changes to model weights constitute training. For example, quantization, the process of reducing the precision of model weights, technically changes the model’s weight values but isn’t considered training.\\n\\nThe term training can often be used in place of pre-training, finetuning, and post- training, which refer to different training phases:\\n\\nPre-training'), Document(id='f6a279b5-7833-42e6-931d-2d0197cc9279', metadata={'start_index': 780012, 'source': 'C:\\\\Users\\\\tejus\\\\OneDrive\\\\Desktop\\\\Full_Stack\\\\ML\\\\RAG_Chatbot\\\\rag_env\\\\data\\\\AI Engineering Building Applications with Foundation Models (Chip Huyen) (Z-Library).pdf'}, page_content='To finetune a model using more than one machine, you’ll need a framework that helps you do distributed training, such as DeepSpeed, PyTorch Distributed, and ColossalAI.\\n\\nFinetuning hyperparameters\\n\\nDepending on the base model and the finetuning method, there are many hyperpara‐ meters you can tune to improve finetuning efficiency. For specific hyperparameters for your use case, check out the documentation of the base model or the finetuning framework you use. Here, I’ll cover a few important hyperparameters that frequently appear.\\n\\nLearning rate. The learning rate determines how fast the model’s parameters should change with each learning step. If you think of learning as finding a path toward a goal, the learning rate is the step size. If the step size is too small, it might take too long to get to the goal. If the step size is too big, you might overstep the goal, and, hence, the model might never converge.'), Document(id='7f455c95-9ac8-4c44-992b-8ad5ab89a835', metadata={'source': 'C:\\\\Users\\\\tejus\\\\OneDrive\\\\Desktop\\\\Full_Stack\\\\ML\\\\RAG_Chatbot\\\\rag_env\\\\data\\\\AI Engineering Building Applications with Foundation Models (Chip Huyen) (Z-Library).pdf', 'start_index': 687981}, page_content='Second, finetuning requires the knowledge of how to train models. You need to eval‐ uate base models to choose one to finetune. Depending on your needs and resources, options might be limited. While finetuning frameworks and APIs can automate many steps in the actual finetuning process, you still need to understand the different training knobs you can tweak, monitor the learning process, and debug when some‐ thing is wrong. For example, you need to understand how an optimizer works, what learning rate to use, how much training data is needed, how to address overfitting/ underfitting, and how to evaluate your models throughout the process.')]\n",
      "Answer: Finetuning involves starting with a pre-existing base model that has some but not all of the desired capabilities. The primary goal is to adapt this model to perform effectively for a specific, new task. It is a method of transfer learning, where knowledge gained from one task is applied to accelerate learning for a related one.\n"
     ]
    }
   ],
   "source": [
    "result = graph.invoke({\"question\":\"Explain Finetuning?\"})\n",
    "\n",
    "print(f\"Question: {result['question']}\")\n",
    "print(f\"Context: {result['context']}\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_env (3.10.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
